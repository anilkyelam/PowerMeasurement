
Generic instructions here:
Hadoop: https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/
Spark: https://www.linode.com/docs/databases/hadoop/install-configure-run-spark-on-top-of-hadoop-yarn-cluster/
	
	
======================== HANDY COMMANDS =======================

Start all: start-dfs.sh && start-yarn.sh
Stop all: stop-yarn.sh && stop-dfs.sh

	
	
==========================  HDFS + YARN SETUP  =========================================================================================
	
1. Install proper java version (latest is not supported by Hadoop yet). 
	Use Java 8 as stated on hadoop website or here: https://stackoverflow.com/questions/52155078/how-to-fix-hadoop-warning-an-illegal-reflective-access-operation-has-occurred-e
	Some of the machines have Java 10 installed. The way to configure is install Java 8 and change default version to 8.
	Java versions can co-exist, so install what is needed and use it for hadoop
	https://stackoverflow.com/questions/12836666/how-to-remove-open-jdk-completely-in-ubuntu
	sudo apt-get install openjdk-8-jdk
	To see available versions: 	update-alternatives --display java
	
2. Download a stable hadoop version. 

3. Create a common user for hadoop on all machines, and a common group to set as a supergroup.
	Username: hadoop
	Password: <Ask>

	useradd hadoop
	addgroup hadoopgroup
	sudo usermod -a -G hadoopgroup ayelam
	
	Set dfs.permissions.superusergroup to this group in hdfs-site.xml
	
	<property>
		<name>dfs.permissions.superusergroup</name>
		<value>hadoopgroup</value>
	</property>
	
	Or set HDFS permissions checks to false as this is dev environment
	<property>
		<name>dfs.permissions.enabled</name>
		<value>false</value>
	</property>

3. Figure out the network names to use for all the nodes. 
	In our case, we have to let the traffic go through a specific network interface. 
	Start with b09-40(10.0.0.1) and b09-38 (10.0.1.1)
	Generate ssh key: 
		ssh-keygen -b 4096
	Copy it to all the machines:
		ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@10.0.0.1
		ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@10.0.1.1
	
4. Copy unzipped hadoop to home folder of hadoop user on all machines (do it from hadoop user context so it retains permissions)

( Errors ignored:
cp: cannot open 'hadoop/share/hadoop/httpfs/tomcat/conf/catalina.properties' for reading: Permission denied
cp: cannot open 'hadoop/share/hadoop/httpfs/tomcat/conf/catalina.policy' for reading: Permission denied
cp: cannot open 'hadoop/share/hadoop/httpfs/tomcat/conf/tomcat-users.xml' for reading: Permission denied
cp: cannot open 'hadoop/share/hadoop/httpfs/tomcat/conf/context.xml' for reading: Permission denied
cp: cannot open 'hadoop/share/hadoop/httpfs/tomcat/conf/web.xml' for reading: Permission denied
cp: cannot open 'hadoop/share/hadoop/kms/tomcat/conf/context.xml' for reading: Permission denied
cp: cannot open 'hadoop/share/hadoop/kms/tomcat/conf/web.xml' for reading: Permission denied
cp: cannot open 'hadoop/share/hadoop/kms/tomcat/conf/catalina.properties' for reading: Permission denied
cp: cannot open 'hadoop/share/hadoop/kms/tomcat/conf/catalina.policy' for reading: Permission denied
cp: cannot open 'hadoop/share/hadoop/kms/tomcat/conf/tomcat-users.xml' for reading: Permission denied
)

	And add the folder to path by adding below line to .profile file (/usr/local/home/hadoop/.profile)
	PATH=/usr/local/home/hadoop/hadoop/bin:/usr/local/home/hadoop/hadoop/sbin:$PATH
	
5. Set JAVA_HOME (on all nodes). Use Java 8 installed in step 1
	Edit ~/hadoop/etc/hadoop/hadoop-env.sh and replace this line:
	vim ~/hadoop/etc/hadoop/hadoop-env.sh
	export JAVA_HOME=${JAVA_HOME} with
	export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre
	
6. Set name node location on each node by updating ~/hadoop/etc/hadoop/core-site.xml
	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://10.0.0.1:9000</value>
        </property>
    </configuration>
	
7. Set path for HDFS, editing hdfs-site.xml
	<configuration>
		<property>
				<name>dfs.namenode.name.dir</name>
				<value>/usr/local/home/hadoop/data/nameNode</value> 
		</property>

		<property>
				<name>dfs.datanode.data.dir</name>
				<value>/usr/local/home/hadoop/data/dataNode</value>
		</property>

		<property>
				<name>dfs.replication</name>
				<value>1</value>
		</property>
	</configuration>
	
8. Set YARN as Job scheduler
	https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/#set-yarn-as-job-scheduler
	cd hadoop/etc/hadoop/
	mv mapred-site.xml.template mapred-site.xml
	vim mapred-site.xml and place the below text:
	<configuration>
		<property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
		</property>
	</configuration>
	
9. Configure YARN
	vim yarn-site-xml
	<configuration>
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <value>127.0.0.1</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
	</configuration>
	
	
10. Configure slaves files and add names of all the slaves, in ~/hadoop/etc/hadoop/slaves
	b09-44.sysnet.ucsd.edu
	b09-42.sysnet.ucsd.edu
	b09-40.sysnet.ucsd.edu
	b09-38.sysnet.ucsd.edu
	b09-36.sysnet.ucsd.edu
	b09-34.sysnet.ucsd.edu
	b09-32.sysnet.ucsd.edu
	b09-30.sysnet.ucsd.edu
	
11. Set memory allocation for containers. Check this config for current cluster!!

	Edit /home/hadoop/hadoop/etc/hadoop/yarn-site.xml and add the following lines:
	<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>61440</value>
	</property>

	<property>
			<name>yarn.scheduler.maximum-allocation-mb</name>
			<value>61440</value>
	</property>

	<property>
			<name>yarn.scheduler.minimum-allocation-mb</name>
			<value>1024</value>
	</property>

	<property>
			<name>yarn.nodemanager.vmem-check-enabled</name>
			<value>false</value>
	</property>
	
	Edit /home/hadoop/hadoop/etc/hadoop/mapred-site.xml and add the following lines: (DONT WORRY ABOUT THIS FOR SPARK + YARN)
	<property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>10240</value>
	</property>

	<property>
			<name>mapreduce.map.memory.mb</name>
			<value>10240</value>
	</property>

	<property>
			<name>mapreduce.reduce.memory.mb</name>
			<value>10240</value>
	</property>
	
12. Copy config files to all the slave nodes
	scp ~/hadoop/etc/hadoop/* (slave_name):/usr/local/home/hadoop/hadoop/etc/hadoop/
	
	
13. Format HDFS and it is now ready to use.
	hdfs namenode -format
	
	
14. Start dfs
	start-dfs.sh
	Accept secondary name node on [0.0.0.0]. Then you can browse hdfs on http://b09-40.sysnet.ucsd.edu:50070
	Datanode bug: Set this setting in hdfs-default.xml to false to turn off data node name resolution. dfs.namenode.datanode.registration.ip-hostname-check
		<property>
			<name>dfs.namenode.datanode.registration.ip-hostname-check</name>
			<value>false</value>
		</property>
	
	
15. To stop dfs: stop-dfs.sh
16. Check: hdfs dfsadmin -report

17. Run YARN: 
	start-yarn.sh
	stop-yarn.sh
	yarn node -list
	yarn application -list
	Web UI at http://b09-40.sysnet.ucsd.edu:8088

18. Adding more nodes. To summarize for b09-36:
	With ayelam on new node:
		sudo adduser hadoop
		sudo addgroup hadoopgroup
		update-alternatives --display java
		sudo apt-get install openjdk-8-jdk
	With hadoop user on b09-30:
		Add new node to ~/hadoop/etc/hadoop/slaves file
		ssh-copy-id -i $HOME/.ssh/id_rsa.pub hadoop@b09-44
		scp .profile b09-44:~/
		scp -r ~/hadoop b09-44:~/
		
	

======================================================  INTEGRATING SPARK ================================================================

1. Download spark on the master node and add it to path
	wget https://www-eu.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz 
	tar -xvf spark-2.3.2-bin-hadoop2.7.tgz
	mv spark-2.3.2-bin-hadoop2.7 spark
	
	In .profile, add
	PATH=/usr/local/home/hadoop/spark/bin:$PATH
	
2. Integrate Spark with YARN
	
	To tell spark about hadoop folder, add below commands to spark
	export HADOOP_CONF_DIR=/usr/local/home/hadoop/hadoop/etc/hadoop
	export SPARK_HOME=/usr/local/home/hadoop/spark
	export LD_LIBRARY_PATH=/usr/local/home/hadoop/hadoop/lib/native:$LD_LIBRARY_PATH
	source .profile
	
	Tell spark to use YARN as cluster manager
	mv $SPARK_HOME/conf/spark-defaults.conf.template $SPARK_HOME/conf/spark-defaults.conf
	vim $SPARK_HOME/conf/spark-defaults.conf , and add below line:
	spark.master    yarn
	
	Set hadoop conf dir variable in spark
	mv $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
	
		
3. Memory allocation for spark-2. Defaults in $SPARK_HOME/conf/spark-defaults.conf
	Default driver node memory allocation (total must be less than YARN max): spark.driver.memory and spark.driver.memoryOverhead
	Default executor node memory allocation (total must be less than YARN max): spark.executor.memory and spark.executor.memoryOverhead
	Used 5g for all for starters
	

4.  Enable monitoring: Logs to HDFS and configure history server
	In $SPARK_HOME/conf/spark-defaults.conf, add following options:
	
	spark.eventLog.enabled  			true
	spark.eventLog.dir 					hdfs://10.0.0.1:9000/spark-logs
	
	spark.history.provider            	org.apache.spark.deploy.history.FsHistoryProvider
	spark.history.fs.logDirectory     	hdfs://10.0.0.1:9000/spark-logs
	spark.history.fs.update.interval  	10s
	spark.history.ui.port             	18080
	
	spark.master                            yarn
	spark.driver.memory                     10g
	spark.driver.memoryOverhead        		50g
	spark.executor.memory                   50g
	spark.executor.memoryOverhead      		50g

	spark.eventLog.enabled                  true
	spark.eventLog.dir                      hdfs://10.0.0.1:9000/spark-logs
	spark.history.provider                  org.apache.spark.deploy.history.FsHistoryProvider
	spark.history.fs.logDirectory           hdfs://10.0.0.1:9000/spark-logs
	spark.history.fs.update.interval        10s
	spark.history.ui.port                   18080
	
	
5. Upload Spark Jars to hdfs for YARN to get access to them and distribute easily
	https://stackoverflow.com/questions/41112801/property-spark-yarn-jars-how-to-deal-with-it
	hdfs dfs -put $SPARK_HOME/jars/* /spark/jars/
	vim $SPARK_HOME/conf/spark-defaults.conf
	spark.yarn.jars                         hdfs://10.0.0.1:9000/spark/jars/*

6. Try spark submit
	spark-submit --class PowerMeasurements.SortLegacy "${SRC_DIR_FULL_PATH}/target/scala-2.11/sparksort_2.11-0.1.jar" \
        yarn "/user/ayelam/sort_inputs/${SIZE_IN_MB}mb.input" "/user/ayelam/sort_outputs/${SIZE_IN_MB}mb.output" \
        "/user/ayelam/sort_stats/${SIZE_IN_MB}mb.stats" > ${RESULTS_DIR_FULL_PATH}/spark.log 2>&1
	
	/usr/local/home/hadoop/spark/bin/spark-submit --class PowerMeasurements.SortLegacy target/scala-2.11/sparksort_2.11-0.1.jar yarn /user/ayelam/1000mb.input /user/ayelam/10001mb.output /user/ayelam/taskstats_${unixstamp}



======= Additional Workload Specific Customizations for HDFS or SPARK ==========================================================

1. Created a "tmpfs" RAM disk at /mnt/ramdisk with 30G on all nodes, and pointed Spark scratch space to use it. 
How: In YARN mode, "yarn.nodemanager.local-dirs" property controls the spark scratch space.  
Why: To avoid writing large Shuffle intermediate (map) outputs to disk

2. Created cache pool for HDFS with 64GB of max memory, to cache input spark files.
How: HDFS Read Caching feature
Why: So that Spark doesn't have to read from disk when loading the input file into an RDD

3. Used custom number of executors and cores-per-executor while submitting spark job.
How:
Why:

4. Changing resource calculator of YARN's capacity scheduler from Default to DominantResourceCalculator.
How: In capacity-scheduler.xml, set "yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DominantResourceCalculator"
Why: Apparently the default calculator just takes memory into consideration when calculating number of containers from input configs, and not CPU: the result being 
	spark.executor.cores or --executor.cores property of spark is not respected.



========================================= INTEGRATING WITH GIRAPH =================================================================

http://giraph.apache.org/quick_start.html
1. Make sure you can run general map-reduce jobs, as that is what Giraph is built on.

2. Clone Giraph repo from GitHub and compile it for YARN and target Hadoop version
sudo git clone https://github.com/apache/giraph.git
  git clone https://github.com/apache/giraph.git
  cd giraph/
  export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
  mvn -Phadoop_yarn -DskipTests -Dhadoop.version=2.8.5 clean install -Dcheckstyle.skip
  (Needed to do a little hack of removing an option from <munge.symbols> in pom.xml for YARN, based on the below link to get compile to work: 
  	http://mail-archives.apache.org/mod_mbox/giraph-user/201501.mbox/%3C54B17196.4040107@hiro-tan.org%3E)

3. At this point, we'll have a Giraph jar file custom built for Hadoop version, and can be executed using "hadoop jar" over MR.

4. Run below command to see options:
	hadoop jar /usr/local/home/hadoop/giraph/giraph-examples/target/giraph-examples-1.3.0-SNAPSHOT-for-hadoop-2.8.5-jar-with-dependencies.jar org.apache.giraph.GiraphRunner -h

5. The example in the above link (SimpleShortestPathsComputation) failed at first. The jar is not being picked up, so had to copy it to yarn libs folder (ON ALL NODES!)
	https://stackoverflow.com/questions/29001491/could-not-find-or-load-main-class-org-apache-giraph-yarn-giraphapplicationmaster

6. Test command
	

Helpful links:
https://blog.cloudera.com/blog/2014/02/how-to-write-and-run-giraph-jobs-on-hadoop/
https://yogin16.github.io/2018/04/09/giraph-hadoop-setup/



========================================= OTHER SOFTWARE INSTALLED ON NODES =====================================================
	
1. Installed sysstat on all nodes to use `sar`
	sudo apt-get install sysstat

2. 
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	