
======= Misc
Copy hadoop conf to rest of the nodes: 
  for d in 38 36 34 32 30 42 44; do scp hadoop/etc/hadoop/file_name   b09-$d:~/hadoop/etc/hadoop/ ; done
  for d in 38 36 34 32 30 42 44; do ssh b09-"$d" "remote-cmd-execute" ; done  


======== Spark submit job commands
spark-submit --class PowerMeasurements.${SCALA_CLASS_NAME} \
        --num-executors 32 --executor-cores 5 --executor-memory 5g --driver-cores 5 --driver-memory 5g \
        "${SRC_DIR_FULL_PATH}/target/scala-2.11/sparksort_2.11-0.1.jar" \
        yarn "/user/ayelam/sort_inputs/${SIZE_IN_MB}mb.input" "/user/ayelam/sort_outputs/${SIZE_IN_MB}mb.output" \
        "/user/ayelam/sort_stats/${SIZE_IN_MB}mb.stats" > ${RESULTS_DIR_FULL_PATH}/spark.log 2>&1

spark-submit --class PowerMeasurements.SortNoDisk \
        --num-executors 32 --executor-cores 4 --executor-memory 5g --driver-cores 5 --driver-memory 5g \
        "bf-cluster/sparksort/node-scripts//target/scala-2.11/sparksort_2.11-0.1.jar" \
        yarn "/user/ayelam/sort_inputs/100000mb.input" 


		
======== Local Spark Commands
spark-submit --class PowerMeasurements.SortNoDisk "D:\SparkSort\target\scala-2.11\sparksort_2.11-0.1.jar" local "D:\SparkSort\input\100mb.input" 


/user/ayelam/sort_inputs/${SIZE_IN_MB}mb.input
/user/shw328/128gb.input < Revert in run_sort_job.sh and prepare_for_experiment.sh


======== Creating a tmpfs RAM Disk
mkdir /mnt/ramdisk
mount -t tmpfs -o size=40G tmpfs /mnt/ramdisk
Add this to /etc/fstab file to make it survive reboots: tmpfs       /mnt/ramdisk tmpfs   nodev,nosuid,noexec,nodiratime,size=30G   0 0

 
======== HDFS Cache Management commands

Two settings:
  - "dfs.client.mmap.enabled" should be "true", which is the default value for this setting.
  - "dfs.datanode.max.locked.memory" - max number of bytes that hadoop uses for caching blocks or HDFS read caching.
Run "ulimit -l" and make sure it is more than the configured setting for max locked memory on each node.

hdfs cacheadmin -listPools
sudo -u hdfs hdfs cacheadmin -addPool anil-cache-pool
hdfs cacheadmin -listDirectives
sudo -u hdfs hdfs cacheadmin -addDirective -path "/user/ayelam/sort_inputs/1000mb.input" -pool anil-cache-pool
sudo -u hdfs hdfs cacheadmin -addDirective -path "/user/ayelam/sort_inputs/5000mb.input" -pool anil-cache-pool
sudo -u hdfs hdfs cacheadmin -addDirective -path "/user/ayelam/sort_inputs/10000mb.input" -pool anil-cache-pool
hdfs cacheadmin -listPools -stats anil-cache-pool

hdfs cacheadmin -addDirective -path "/user/ayelam/sort_inputs/100000mb.input" -pool cache-pool

sudo -u hdfs hdfs cacheadmin -removeDirective <directive-id>
sudo -u hdfs dfs cacheadmin -removePool anil-cache-pool


======== HDFS Commands
Get data block placement for a file: hdfs fsck /user/ayelam/sort_inputs/20000mb.input -files -blocks -locations
Set replication of a file or dir: hdfs dfs -setrep 1 /user/ayelam/sort_inputs/
Add a file with certain repl. factor: hdfs dfs -D dfs.replication=2  -put 20000mb.input /user/ayelam/sort_inputs/
Algo to SortLegacy


hdfs dfs -setrep 3 /user/ayelam/sort_inputs/


========= Giraph job submit commands

Help:
$HADOOP_HOME/bin/hadoop jar $GIRAPH_HOME/giraph-examples/target/giraph-examples-1.3.0-SNAPSHOT-for-hadoop-2.8.5-jar-with-dependencies.jar org.apache.giraph.GiraphRunner -help

Submit pagerank job:
hadoop jar $GIRAPH_HOME/giraph-examples/target/giraph-examples-1.3.0-SNAPSHOT-for-hadoop-2.8.5-jar-with-dependencies.jar org.apache.giraph.GiraphRunner org.apache.giraph.examples.SimplePageRankComputation -vif org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat -vip /tmp/tiny_graph.txt -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat -op /tmp/pageranks4 -w 1 --masterCompute org.apache.giraph.examples.SimplePageRankComputation\$SimplePageRankMasterCompute --yarnjars giraph-examples-1.3.0-SNAPSHOT-for-hadoop-2.8.5-jar-with-dependencies.jar


======= Firewall ports opened
50070 - HDFS namenode
50075 - HDFS data node
4040, 8088, 18089 - Spark ports

sudo iptables -A INPUT -s 137.110.222.168 -p tcp --dport 4040 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
sudo iptables -I INPUT 12 -s ayelam.sysnet.ucsd.edu -p tcp --dport 4040 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied21
sudo iptables -I INPUT 9 -s ayelam.sysnet.ucsd.edu -p tcp --dport 8088 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT			// ccied28
sudo iptables -I INPUT 11 -s ayelam.sysnet.ucsd.edu -p tcp --dport 18089 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied26
sudo iptables -I INPUT 9 -s ayelam.sysnet.ucsd.edu -p tcp --dport 50070 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied30
sudo iptables -I INPUT 9 -s ayelam.sysnet.ucsd.edu -p tcp --dport 50070 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied9
sudo iptables -I INPUT 9 -s ayelam.sysnet.ucsd.edu -p tcp --dport 50075 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied29


======= Monitoring memory bandwidth
Using PQoS provided by intel-cat-cmt tools: https://github.com/intel/intel-cmt-cat



======= Changes for Shuting's experiments
Remove all caching stuff
FILE_PATH_HDFS="/user/ayelam/sort_inputs/${INPUT_SIZE_MB}mb.input" in prepare_for_experiment.sh
num-executors AND input file (/user/ayelam/sort_inputs/${SIZE_IN_MB}mb.input) in run_spark_job.sh
limit_executors in run_experiments.py



======== Experiments 
    # 12/10 22:00 to 12/11 14:00        ---> No output (Caching, 3 replicas)
    # 12/06 00:00 to 12/08 00:00        ---> No output (No caching, 3 replicas)
    # 11/29 00:00 to 12/01 00:00        ---> Legacy sort (3 replicas)
    # 2019/01/15 11:00 to 01/17 15:00   ---> Legacy sort (No Trash, one replica)
    # 12/21 00:00 to 12/22 00:00        ---> No output (Caching, one replica)
    # 2019/01/17 20:00 to - 01/18 13:00 ---> No output (No Trash, one replica, more steps)
    # 2019/01/18 14:00:00 to - 2019-01-20 00:00:00 ---> No output (Caching, one replica, No Trash, more steps)
    # 2019-01-20 23:00:00 to                ---> Legacy sort (No Trash, 3 replicas)


  