
======= Hadoop cluster access URLs
Hdfs overview: http://b09-40.sysnet.ucsd.edu:50070/dfshealth.html#tab-overview
Mapred/YARN job history server: b09-40.sysnet.ucsd.edu:19888 or b09-40.sysnet.ucsd.edu:10020

======= Misc
Copy hadoop conf to rest of the nodes: 
  for d in 38 36 34 32 30 42 44; do scp hadoop/etc/hadoop/file_name   b09-$d:~/hadoop/etc/hadoop/ ; done
  for d in 38 36 34 32 30 42 44; do ssh b09-"$d" "remote-cmd-execute" ; done  


======== Spark submit job commands
spark-submit --class PowerMeasurements.${SCALA_CLASS_NAME} \
        --num-executors 32 --executor-cores 5 --executor-memory 5g --driver-cores 5 --driver-memory 5g \
        "${SRC_DIR_FULL_PATH}/target/scala-2.11/sparksort_2.11-0.1.jar" \
        yarn "/user/ayelam/sort_inputs/${SIZE_IN_MB}mb.input" "/user/ayelam/sort_outputs/${SIZE_IN_MB}mb.output" \
        "/user/ayelam/sort_stats/${SIZE_IN_MB}mb.stats" > ${RESULTS_DIR_FULL_PATH}/spark.log 2>&1

spark-submit --class PowerMeasurements.SortNoDisk \
        --num-executors 32 --executor-cores 4 --executor-memory 5g --driver-cores 5 --driver-memory 5g \
        "bf-cluster/sparksort/node-scripts//target/scala-2.11/sparksort_2.11-0.1.jar" \
        yarn "/user/ayelam/sort_inputs/100000mb.input" 


		
======== Local Spark Commands
spark-submit --class PowerMeasurements.SortNoDisk "D:\SparkSort\target\scala-2.11\sparksort_2.11-0.1.jar" local "D:\SparkSort\input\100mb.input" 


======== Creating a tmpfs RAM Disk
mkdir /mnt/ramdisk
mount -t tmpfs -o size=40G tmpfs /mnt/ramdisk
Add this to /etc/fstab file to make it survive reboots: tmpfs       /mnt/ramdisk tmpfs   nodev,nosuid,noexec,nodiratime,size=30G   0 0

 
======== HDFS Cache Management commands
Two settings:
  - "dfs.client.mmap.enabled" should be "true", which is the default value for this setting.
  - "dfs.datanode.max.locked.memory" - max number of bytes that hadoop uses for caching blocks or HDFS read caching.
Run "ulimit -l" and make sure it is more than the configured setting for max locked memory on each node.

hdfs cacheadmin -listPools
sudo -u hdfs hdfs cacheadmin -addPool anil-cache-pool
hdfs cacheadmin -listDirectives
sudo -u hdfs hdfs cacheadmin -addDirective -path "/user/ayelam/sort_inputs/1000mb.input" -pool anil-cache-pool
sudo -u hdfs hdfs cacheadmin -addDirective -path "/user/ayelam/sort_inputs/5000mb.input" -pool anil-cache-pool
sudo -u hdfs hdfs cacheadmin -addDirective -path "/user/ayelam/sort_inputs/10000mb.input" -pool anil-cache-pool
sudo -u hdfs hdfs cacheadmin -addDirective -path "/user/ayelam/graph_inputs/1000mb.input" -pool cache-pool

hdfs cacheadmin -listPools -stats cache-pool
hdfs cacheadmin -addDirective -path "/user/ayelam/graph_inputs/darwini-2b-edges" -pool cache-pool

sudo -u hdfs hdfs cacheadmin -removeDirective <directive-id>
sudo -u hdfs dfs cacheadmin -removePool anil-cache-pool


======== HDFS data replication Commands
Get data block placement for a file: hdfs fsck /user/ayelam/sort_inputs/20000mb.input -files -blocks -locations
Set replication of a file or dir: hdfs dfs -setrep 1 /user/ayelam/sort_inputs/
Add a file with certain repl. factor: hdfs dfs -D dfs.replication=2  -put 20000mb.input /user/ayelam/sort_inputs/
hdfs dfs -setrep 3 /user/ayelam/sort_inputs/

hdfs dfs  -D dfs.replication=2  -put /usr/local/home/ayelam/darwini-2b-edges/  /user/ayelam/graph_inputs/
hdfs dfs -setrep 1 /user/ayelam/graph_inputs/darwini-2b-edges


========= Giraph job submit commands
Help:
$HADOOP_HOME/bin/hadoop jar $GIRAPH_HOME/giraph-examples/target/giraph-examples-1.3.0-SNAPSHOT-for-hadoop-2.8.5-jar-with-dependencies.jar org.apache.giraph.GiraphRunner -help

Submit pagerank job:
hadoop jar $GIRAPH_HOME/giraph-examples/target/giraph-examples-1.3.0-SNAPSHOT-for-hadoop-2.8.5-jar-with-dependencies.jar org.apache.giraph.GiraphRunner org.apache.giraph.examples.SimplePageRankComputation -vif org.apache.giraph.io.formats.LongDoubleFloatTextInputFormat -vip /user/ayelam/graph_inputs/darwini-10m-edges -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat -op /tmp/darwini-10m-edges --masterCompute org.apache.giraph.examples.SimplePageRankComputation\$SimplePageRankMasterCompute --yarnjars giraph-examples-1.3.0-SNAPSHOT-for-hadoop-2.8.5-jar-with-dependencies.jar --yarnheap 65536 -w 2

giraph /usr/local/home/hadoop/giraph/giraph-examples/target/giraph-examples-1.3.0-SNAPSHOT-for-hadoop-2.5.1-jar-with-dependencies.jar org.apache.giraph.examples.SimplePageRankComputation -vif org.apache.giraph.io.formats.LongDoubleFloatTextInputFormat -vip /user/ayelam/graph_inputs/darwini-2b-edges -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat -op /tmp/darwini-2b-edges11 --mc org.apache.giraph.examples.SimplePageRankComputation\$SimplePageRankMasterCompute  -w 7 -ca giraph.numComputeThreads=5,giraph.numInputThreads=5


======= Monitoring memory bandwidth
Using PQoS provided by intel-cat-cmt tools: https://github.com/intel/intel-cmt-cat


======= Network traffic breakdown
Per-flow packet trace using tShark:
sudo tshark -i enp101s0 -t a -T fields -e frame.time_epoch -e ip.src -e tcp.srcport -e ip.dst -e tcp.dstport -e frame.len -Q out "tcp" > net_usage_break_down

To put temp file and final output files in ramdisk (and not disk) so the disk does not cause packet drops in capture. (although it can only support smaller sizes, tmp pacp files for 100GB input sort takes up memory)
sudo tshark -i enp101s0 -t a -T fields -e frame.time_epoch -e ip.src -e tcp.srcport -e ip.dst -e tcp.dstport -e frame.len -B 100 -w /mnt/ramdisk/tshark_temp_file -Q out "ip" > /mnt/ramdisk/net_usage_breakdown

================== ARCHIVED ==========================================================================================================

======= Firewall ports opened
50070 - HDFS namenode
50075 - HDFS data node
4040, 8088, 18089 - Spark ports

sudo iptables -A INPUT -s 137.110.222.168 -p tcp --dport 4040 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT
sudo iptables -I INPUT 12 -s ayelam.sysnet.ucsd.edu -p tcp --dport 4040 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied21
sudo iptables -I INPUT 9 -s ayelam.sysnet.ucsd.edu -p tcp --dport 8088 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT			// ccied28
sudo iptables -I INPUT 11 -s ayelam.sysnet.ucsd.edu -p tcp --dport 18089 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied26
sudo iptables -I INPUT 9 -s ayelam.sysnet.ucsd.edu -p tcp --dport 50070 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied30
sudo iptables -I INPUT 9 -s ayelam.sysnet.ucsd.edu -p tcp --dport 50070 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied9
sudo iptables -I INPUT 9 -s ayelam.sysnet.ucsd.edu -p tcp --dport 50075 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT		// ccied29


======== Experiments 
    # 12/10 22:00 to 12/11 14:00        ---> No output (Caching, 3 replicas)
    # 12/06 00:00 to 12/08 00:00        ---> No output (No caching, 3 replicas)
    # 11/29 00:00 to 12/01 00:00        ---> Legacy sort (3 replicas)
    # 2019/01/15 11:00 to 01/17 15:00   ---> Legacy sort (No Trash, one replica)
    # 12/21 00:00 to 12/22 00:00        ---> No output (Caching, one replica)
    # 2019/01/17 20:00 to - 01/18 13:00 ---> No output (No Trash, one replica, more steps)
    # 2019/01/18 14:00:00 to - 2019-01-20 00:00:00 ---> No output (Caching, one replica, No Trash, more steps)
    # 2019-01-20 23:00:00 to                ---> Legacy sort (No Trash, 3 replicas)


======= Changes for Shuting's experiments
Remove all caching stuff
FILE_PATH_HDFS="/user/ayelam/sort_inputs/${INPUT_SIZE_MB}mb.input" in prepare_for_experiment.sh
num-executors AND input file (/user/ayelam/sort_inputs/${SIZE_IN_MB}mb.input) in run_spark_job.sh
limit_executors in run_experiments.py
